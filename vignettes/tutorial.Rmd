---
title: "Project 3: STAT302package Tutorial"
output: rmarkdown::html_vignette
author: Eunice Zhang Ziyi Li
vignette: >
  %\VignetteIndexEntry{STAT302package Tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(STAT302package)
library(dplyr)
library(tidyr)
library(ggplot2)
library(randomForest)
```

### Introduction 

Our package,"STAT302package" utilizes Hypothesis Testing and Statistical Prediction Algorithms (K-nearest neighbor and Random Forest) to produce inference and prediction. It includes four functions. The first function is called `my_t.test()`, which performs a sample t-test,including one and two sided tests in R. The result of `my_t.test` return a `list` of four elements: t-stat, degree of freedom, test method and p value. The second function is called `my_lm()`, which fits a linear model in R and returns a `table` with rows for each coefficient (including the `(Intercept)`) and columns for the `Estimate`, `Std. Error`, `t value`, and `Pr(>|t|)`. The third function `my_knn_cv` and fourth function `my_rf_cv` utilize the data `penguins` from the package `palmerpenguins` to predict the species and body mass of penguins separately based on cross-validation algorithm and random forest.The documentation of each function can be accessed through ?function_name().


install the development version directly from GitHub. Before you install from Github, make sure you have the `devtools` package. 
```{r,eval=FALSE}
library(STAT302package)
# install.packages("devtools")
# devtools::install_github("eunicezh/corncob")

```

### T-test

Interpretation: 

In the majority of analyses, an alpha of 0.05 is used as the cutoff for significance. When p value is less than 0.05, we reject the null hypothesis that there’s no significant difference between the means and conclude that a significant difference does exist. In the following table, we have done 3 hypothesis tests for the mean of dependent variable - life expectancy.

For the first t-test, we observe P = 0.09322877, so we do not have sufficient evidence to reject the null hypothesis and conclude that the true mean life expectancy is not equal to 60.

For the second t-test, we observe P = 0.04661438, so we can reject the null hypothesis and conclude that there’s no difference between the means and a significant difference does exist. the true mean life expectancy is greater than 60.

For the third t-test, we observe P = 0.9533856,so we do not have sufficient evidence to reject the null hypothesis. Therefore, we cannot conclude that the true mean life expectancy is smaller than 60.

```{r}
# extract the data
demonstration_data <- my_gapminder$lifeExp

# p_value cut-off of alpha
alpha = 0.05

# Demonstrate a test of the hypothesis (Two-Sided)
result_ts <- my_t.test(demonstration_data, "two.sides", 60)

# Demonstrate a test of the hypothesis (less)
result_less <- my_t.test(demonstration_data, "less", 60)

# Demonstrate a test of the hypothesis (greater)
result_greater <- my_t.test(demonstration_data, "greater", 60)

test_result <- data.frame("two sides" = result_ts$p_val, "less" = result_less$p_val, "greater" = result_greater$p_val, "alpha" = alpha )
rownames(test_result) <- "p value"
test_result 
```


### linear Regression

In this section, we use the linear regression to fit our model as following:

```{r}
# create the formula used for my_lm
demo_regression <- lm(formula = demonstration_data ~ my_gapminder$gdpPercap + my_gapminder$continent, data = my_gapminder)
demo_regression
```

The coefficient of gdpPercap is 4.452704e-04, which means that for every one unit of increase in GDP per capita, there will be 4.452704e-04 increase in life expectancy, which doesn't show a significant impact on the life expectancy. 


```{r}
# demonstrate the linear regression
output_regression <- my_lm(demo_regression, data = my_gapminder)
output_regression
```

In the next step, we make a two sided hypothesis test testing whether this value is significantly significant. 

\begin{align}
H_0: p& = 4.452704e-04
H_a: p& \neq 4.452704e-04
\end{align}

According to our result, p-value is 8.552893e-73, which is significantly smaller than 0.05. Therefore, we have enough evidence to reject that the null hypothesis that the mean of gdpPerCap is 4.452704e-04.

```{r}
# extract the estimates for gdpPercap
est_gdp <- output_regression[2,4]

test_result2 <- data.frame("two sides" = est_gdp, "alpha" = alpha )
rownames(test_result2) <- "p value"
test_result2
```

In the following plot, we plot the actual values vs fitted value of Life expectancy. The X-axis represents the fitted, and the Y axis represents the actual value. According to the plot, we could see that not all points for each continent fits the linear line closely. Instead, points from each continent group compose like vertical lines, which means that our model might has a low R-squared value. Also, there are some outliers. 

```{r}
# calculate y-hat
beta <- as.matrix(output_regression[,1])
alpha <- model.matrix(demonstration_data ~ my_gapminder$gdpPercap + my_gapminder$continent, data = my_gapminder)
y_hat <- alpha %*% beta

# plot the graph
continent <- my_gapminder$continent
my_df <- data.frame(actual = my_gapminder$lifeExp, fitted = y_hat) 
actual_fitted <- ggplot(my_df, aes(x = fitted, y = actual, color = continent)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, col = "red", lty = 2) + 
  theme_bw(base_size = 15) +
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5))

actual_fitted 
```


### k-Nearest Neighbors Cross-Validation

In this section, we utilize the penguins data from the package `palmerpenguins` to predict the species of penguins and compute the corresponding CV and traing error associated with different value of K-nearest neighbor.

Based on the table  we could see a clear pattern that CV error and Training Error increases as K increases, but the scale of training error is much smaller than the CV error. 

The smaller the error is, the more precised our prediction will be, Therefore, we would choose 1 k-nearest neighbor as our model since it has the smallest error for both CV and Training error.

However, in practice we would choose different number for our K nearest neighbor, since there is a bias-variance tradeoff. The smaller the bias is, the larger the variance will be. We train our model to be more specific to our data, so the bias would decrease but it will cause our model to be overfit. Therefore, we might choose K=5 in practice. 

Cross-Validation is a re-sampling process that splits our data into N folds. number of Training data would be N-1, and the remaining data would be the test data. Then, test data and training data would be switched until all folds have been test data. This process utilize the limited data more efficiently since each data could be considered as training and test data. Also, this could help us better estimate the performance of our data. 

```{r}
# compute the cv and train error
set.seed(121)
my_penguins <- na.omit(my_penguins)

cv_result_list <- rep(NA,10)
train_result_list <- rep(NA,10)
for (i in 1:10) {
  cv_result <- my_knn_cv(5, my_penguins[,3:6],my_penguins[[1]], i)
  cv_result_list[i] <- cv_result[[2]]
  train_result <-sum(as.numeric(my_penguins$species != cv_result[[1]])) / nrow(my_penguins)
  train_result_list[i] <- train_result
}

# Make a table composed of CV error and Training Error
err_table <- data.frame("CV Error" = cv_result_list, "Training Error" = train_result_list)
rownames(err_table) <- c(1:10)
err_table
```


### Random Forest Cross-Validation

In this section, we will use random forest to test our penguins data. We run the 30 simulations of the CV estimated MSE when K folds = 2, 5, and 10. Then, we make a boxplot of these 30 simulations and compute the mean and standard deviation of these values.

In the boxplot and table, we could see the standard deviation for k=2 is the largest and mean is smallest when k=10, which corresponds to the bias-variance tradeoff we mentioned in the last section. Since we train our model to be more specific to our data,  the bias would decrease but it will cause our model to be overfit. 

```{r}
# compute CV estimated MSE when k = 2
MSE_2 <- data.frame()
for(i in 1:30) {
  result_2 <- my_rf_cv(k = 2)
  MSE_2[i,1] <- unlist(result_2)
}
colnames(MSE_2) <- c("MSE")
rownames(MSE_2) <- seq(1,30)


#Calculate CV estimated MSE when k = 5
MSE_5 <- data.frame()
for(i in 1:30) {
  result_5 <- my_rf_cv(k = 5)
  MSE_5[i,1] <- unlist(result_5)
}
colnames(MSE_5) <- c("MSE")
rownames(MSE_5) <- seq(1,30)


#Calculate CV estimated MSE when k = 10
MSE_10 <- data.frame()
for(i in 1:30) {
  result_10 <- my_rf_cv(k = 10)
  MSE_10[i,1] <- unlist(result_10)
}
colnames(MSE_10) <- c("MSE")
rownames(MSE_10) <- seq(1,30)



```

```{r}
# plot the boxplot when k= 2, 5, and 10
folds <- rep(c(2, 5, 10), each = 30) %>% as.factor()
all_k <- rbind(MSE_2, MSE_5, MSE_10)
df <- data.frame(all_k, folds)
boxplot_cv <- ggplot(data = df,aes(x = unlist(folds),y = unlist(all_k),group = unlist(folds),fill = folds)) +
  geom_boxplot() +
  theme_bw() +
  labs(x = "Folds", y = "MSE", title = "CV estimated MSE with k=2,5,10")
  theme(plot.title = element_text(hjust = 0.5))
boxplot_cv 
```

```{r}
# calculate the average of CV estimates 
avg2 <- mean(as.numeric(MSE_2[,1]))
avg5 <- mean(as.numeric(MSE_5[,1]))
avg10 <- mean(as.numeric(MSE_10[,1]))

# calculate the sd of CV estimates
sd2 <- sd(as.numeric(MSE_2[,1]))
sd5 <- sd(as.numeric(MSE_5[,1]))
sd10 <- sd(as.numeric(MSE_10[,1]))

cv_table <- data.frame("k value" = c(2, 5, 10), "Average" = c(avg2,avg5,avg10), "Standard Deviation" = c(sd2,sd5,sd10))
cv_table
```

### conclusion

In this vignette, we demonstrates example usage of all main functions. We use the p_value to test the statistical significance of mean of life expectancy, do the linear regression to find the independent variables for the life expectancy in the first two sections. In the next two sections, we use basic machine learning mechanisms to evaluate our model based on the CV error and Training error. Also, we find the tradeoff between the bias and variance based on our calculation. The larger the bias is, the smaller the variance will be. 
